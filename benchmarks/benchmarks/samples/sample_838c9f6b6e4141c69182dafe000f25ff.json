{
    "title": "Redo embeddings API process with increased limits",
    "description": "",
    "id": "838c9f6b6e4141c69182dafe000f25ff",
    "parent_id": "",
    "repo": "https://github.com/AbanteAI/mentat",
    "merge_base": "3195310a081009bc67ac7fa5e9870b3b95a12fcf",
    "diff_merge_base": "",
    "diff_active": "",
    "message_history": [],
    "message_prompt": "I've just realized that I misunderstood the Embeddings API limits. What I calculate as the max_model_tokens is not the maximum per API call, it's the maximum per INPUT (list item) in an API call, and each call's input batch size is 2048. I need to refactor how I calculate batches, but also I'll rarely need to make more than one API call, so I can simplify my code to just basic (serial) batching. You should (1) In embeddings.py, add the embedding api batch size as a constant (2) get rid of _batch_ffd and _fetch_embeddings, (3) continue the features/checksums/tokens lists pattern through the API process as texts/checksums/tokens; if you need more than one batch, just index those directly. (4) Remove test_batch_ffd, and (5) Update similarity_score test to match changes above. ",
    "message_edit": "I will update `embeddings.py` to remove the `_batch_ffd` and `_fetch_embeddings` functions and refactor the `get_feature_similarity_scores` function to handle the new batch size limit. I will also update the `tests/embeddings_test.py` file to remove the `test_batch_ffd` test and update the `test_get_feature_similarity_scores` test to match the changes.\n\nSteps:\n1. Add the embedding API batch size as a constant in `embeddings.py`.\n2. Remove the `_batch_ffd` function from `embeddings.py`.\n3. Remove the `_fetch_embeddings` function from `embeddings.py`.\n4. Refactor the `get_feature_similarity_scores` function to handle the new batch size limit.\n5. Remove the `test_batch_ffd` test from `tests/embeddings_test.py`.\n6. Update the `test_get_feature_similarity_scores` test in `tests/embeddings_test.py` to match the changes in `embeddings.py`.",
    "context": [
        "mentat/embeddings.py",
        "tests/embeddings_test.py"
    ],
    "diff_edit": "diff --git a/mentat/embeddings.py b/mentat/embeddings.py\nindex f6bcc4f..f4442e1 100644\n--- a/mentat/embeddings.py\n+++ b/mentat/embeddings.py\n@@ -1,2 +1 @@\n-import asyncio\n import json\n@@ -20,3 +19,3 @@ from mentat.utils import mentat_dir_path, sha256\n \n-MAX_SIMULTANEOUS_REQUESTS = 10\n+EMBEDDINGS_API_BATCH_SIZE = 2048\n \n@@ -69,31 +68,2 @@ database = EmbeddingsDatabase()\n \n-def _batch_ffd(data: dict[str, int], batch_size: int) -> list[list[str]]:\n-    \"\"\"Batch files using the First Fit Decreasing algorithm.\"\"\"\n-    # Sort the data by the length of the strings in descending order\n-    sorted_data = sorted(data.items(), key=lambda x: x[1], reverse=True)\n-    batches = list[list[str]]()\n-    for key, value in sorted_data:\n-        # Place each item in the first batch that it fits in\n-        placed = False\n-        for batch in batches:\n-            if sum(data[k] for k in batch) + value <= batch_size:\n-                batch.append(key)\n-                placed = True\n-                break\n-        if not placed:\n-            batches.append([key])\n-    return batches\n-\n-\n-embedding_api_semaphore = asyncio.Semaphore(MAX_SIMULTANEOUS_REQUESTS)\n-\n-\n-async def _fetch_embeddings(model: str, batch: list[str]):\n-    ctx = SESSION_CONTEXT.get()\n-\n-    async with embedding_api_semaphore:\n-        response = await ctx.llm_api_handler.call_embedding_api(batch, model)\n-        return response\n-\n-\n def _cosine_similarity(v1: list[float], v2: list[float]) -> float:\n@@ -117,2 +87,4 @@ async def get_feature_similarity_scores(\n     embedding_model = session_context.config.embedding_model\n+    llm_api_handler = session_context.llm_api_handler\n+\n     max_model_tokens = model_context_size(embedding_model)\n@@ -125,12 +97,12 @@ async def get_feature_similarity_scores(\n \n-    # Make a checksum:content dict of all items that need to be embedded\n-    items_to_embed = dict[str, str]()\n-    items_to_embed_tokens = dict[str, int]()\n+    # Make lists of all items that need to be embedded\n+    embed_texts = list[str]()\n+    embed_checksums = list[str]()\n+    embed_tokens = list[int]()\n     prompt_checksum = sha256(prompt)\n-    num_prompt_tokens = 0\n     if not database.exists(prompt_checksum):\n-        items_to_embed[prompt_checksum] = prompt\n-        items_to_embed_tokens[prompt_checksum] = count_tokens(\n-            prompt, embedding_model, False\n-        )\n+        prompt_tokens = count_tokens(prompt, embedding_model, False)\n+        embed_texts.append(prompt)\n+        embed_checksums.append(prompt_checksum)\n+        embed_tokens.append(prompt_tokens)\n     for feature, checksum, token in zip(features, checksums, tokens):\n@@ -140,6 +112,5 @@ async def get_feature_similarity_scores(\n             feature_content = feature.get_code_message()\n-            # Remove line numbering\n-            items_to_embed[checksum] = \"\\n\".join(feature_content)\n-            items_to_embed_tokens[checksum] = token\n-            num_prompt_tokens += token\n+            embed_texts.append(\"\\n\".join(feature_content))\n+            embed_checksums.append(checksum)\n+            embed_tokens.append(token)\n \n@@ -153,6 +124,6 @@ async def get_feature_similarity_scores(\n     else:\n-        expected_cost = (num_prompt_tokens / 1000) * cost[0]\n+        expected_cost = (sum(embed_tokens) / 1000) * cost[0]\n         if expected_cost > 1.0:\n             stream.send(\n-                f\"Embedding {num_prompt_tokens} tokens will cost ${cost[0]:.2f}.\"\n+                f\"Embedding {sum(embed_tokens)} tokens will cost ${cost[0]:.2f}.\"\n                 \" Continue anyway?\"\n@@ -164,20 +135,21 @@ async def get_feature_similarity_scores(\n     # Fetch embeddings in batches\n-    batches = _batch_ffd(items_to_embed_tokens, max_model_tokens)\n-\n-    tasks = list[tuple[asyncio.Task[list[list[float]]], list[str]]]()\n-    for batch in batches:\n-        batch_content = [items_to_embed[k] for k in batch]\n-        task = asyncio.create_task(_fetch_embeddings(embedding_model, batch_content))\n-        tasks.append((task, batch))\n-    for i, (task, batch) in enumerate(tasks):\n+    n_batches = len(embed_texts) // EMBEDDINGS_API_BATCH_SIZE + 1\n+    for batch in range(n_batches):\n         if loading_multiplier:\n             stream.send(\n-                f\"Fetching embeddings, batch {i+1}/{len(batches)}\",\n+                f\"Fetching embeddings, batch {batch+1}/{n_batches}\",\n                 channel=\"loading\",\n-                progress=(100 / len(batches)) * loading_multiplier,\n+                progress=(100 / n_batches) * loading_multiplier,\n             )\n         start_time = default_timer()\n-        response = await task\n+        i_start, i_end = (\n+            batch * EMBEDDINGS_API_BATCH_SIZE,\n+            (batch + 1) * EMBEDDINGS_API_BATCH_SIZE,\n+        )\n+        _texts = embed_texts[i_start:i_end]\n+        _checksums = embed_checksums[i_start:i_end]\n+        _tokens = embed_tokens[i_start:i_end]\n+        response = await llm_api_handler.call_embedding_api(_texts, embedding_model)\n         cost_tracker.log_api_call_stats(\n-            sum(items_to_embed_tokens[k] for k in batch),\n+            sum(_tokens),\n             0,\n@@ -186,3 +158,3 @@ async def get_feature_similarity_scores(\n         )\n-        database.set({k: v for k, v in zip(batch, response)})\n+        database.set({k: v for k, v in zip(_checksums, response)})\n \ndiff --git a/tests/embeddings_test.py b/tests/embeddings_test.py\nindex 1f241b8..f338599 100644\n--- a/tests/embeddings_test.py\n+++ b/tests/embeddings_test.py\n@@ -5,11 +5,3 @@ import pytest\n from mentat.code_feature import CodeFeature\n-from mentat.embeddings import _batch_ffd, get_feature_similarity_scores\n-\n-\n-def test_batch_ffd():\n-    data = {\"a\": 4, \"b\": 5, \"c\": 3, \"d\": 2}\n-    batch_size = 6\n-    result = _batch_ffd(data, batch_size)\n-    expected = [[\"b\"], [\"a\", \"d\"], [\"c\"]]\n-    assert result == expected\n+from mentat.embeddings import get_feature_similarity_scores\n \n",
    "test_command": "",
    "version": "0.1.0"
}