{
    "title": "Replace use_llm with config attribute",
    "description": "",
    "id": "e0cc3cf966de48b5be1b34aef2ce80ce",
    "parent_id": "",
    "repo": "https://github.com/AbanteAI/mentat",
    "merge_base": "9498bdde0dcdeafcaaac6ab148bdb599f3d7548c",
    "diff_merge_base": "",
    "diff_active": "diff --git a/mentat/config.py b/mentat/config.py\nindex 8d52d78..4f42393 100644\n--- a/mentat/config.py\n+++ b/mentat/config.py\n@@ -118,2 +118,17 @@ class Config:\n     )\n+    llm_feature_filter: int = attr.field(  # pyright: ignore\n+        default=0,\n+        metadata={\n+            \"description\": (\n+                \"Send this many tokens of auto-context-selected code files to an LLM\"\n+                \" along with the user_prompt to post-select only files which are\"\n+                \" relevant to the task. Post-files will then be sent to the LLM again\"\n+                \" to respond to the user's prompt.\"\n+            ),\n+            \"abbreviation\": \"l\",\n+            \"const\": 5000,\n+        },\n+        converter=int,\n+        validator=validators.ge(0),  # pyright: ignore\n+    )\n \n",
    "message_history": [],
    "message_prompt": "Replace code_context.use_llm with config.llm_feature_filter throughout the code. (1) The DefaultFilter should make the boolean value use_llm from config.llm.. in apply, (2) the LLMFeatureFilter should choose the lesser of that or `model_context_size` to send to the llm, and (3) remove the mentions of code_context.use_llm from the tests and (4) remove it from the code_context altogether. ",
    "message_edit": "I will replace the `code_context.use_llm` attribute with `config.llm_feature_filter` throughout the code. This will involve:\n\n1. Modifying the `DefaultFilter` class to use `config.llm_feature_filter` instead of `use_llm`.\n2. Updating the `LLMFeatureFilter` class to use the minimum of `config.llm_feature_filter` and `model_context_size`.\n3. Removing mentions of `code_context.use_llm` from the tests.\n4. Removing the `use_llm` attribute from the `CodeContext` class.",
    "context": [
        "mentat/feature_filters/default_filter.py",
        "mentat/code_context.py",
        "tests/code_context_test.py",
        "mentat/config.py",
        "mentat/feature_filters/llm_feature_filter.py"
    ],
    "diff_edit": "diff --git a/mentat/code_context.py b/mentat/code_context.py\nindex ec4d207..362682f 100644\n--- a/mentat/code_context.py\n+++ b/mentat/code_context.py\n@@ -106,4 +106,2 @@ class CodeContext:\n \n-    use_llm: bool = False\n-\n     async def get_code_message(\n@@ -164,3 +162,2 @@ class CodeContext:\n                 auto_tokens,\n-                self.use_llm,\n                 prompt,\ndiff --git a/mentat/feature_filters/default_filter.py b/mentat/feature_filters/default_filter.py\nindex e2d4b74..be8f6ab 100644\n--- a/mentat/feature_filters/default_filter.py\n+++ b/mentat/feature_filters/default_filter.py\n@@ -15,3 +15,2 @@ class DefaultFilter(FeatureFilter):\n         max_tokens: int,\n-        use_llm: bool = False,\n         user_prompt: Optional[str] = None,\n@@ -21,3 +20,2 @@ class DefaultFilter(FeatureFilter):\n         self.max_tokens = max_tokens\n-        self.use_llm = use_llm\n         self.user_prompt = user_prompt or \"\"\n@@ -29,8 +27,9 @@ class DefaultFilter(FeatureFilter):\n \n+        use_llm = ctx.config.llm_feature_filter > 0\n         if ctx.config.auto_context_tokens > 0 and self.user_prompt != \"\":\n             features = await EmbeddingSimilarityFilter(\n-                self.user_prompt, (0.5 if self.use_llm else 1) * self.loading_multiplier\n+                self.user_prompt, (0.5 if use_llm else 1) * self.loading_multiplier\n             ).filter(features)\n \n-        if self.use_llm:\n+        if use_llm:\n             try:\ndiff --git a/mentat/feature_filters/llm_feature_filter.py b/mentat/feature_filters/llm_feature_filter.py\nindex 3a80c76..530a067 100644\n--- a/mentat/feature_filters/llm_feature_filter.py\n+++ b/mentat/feature_filters/llm_feature_filter.py\n@@ -54,2 +54,3 @@ class LLMFeatureFilter(FeatureFilter):\n             )\n+        context_size = min(context_size, config.llm_feature_filter)\n         system_prompt = read_prompt(self.feature_selection_prompt_path)\ndiff --git a/tests/code_context_test.py b/tests/code_context_test.py\nindex 41877d6..63808e1 100644\n--- a/tests/code_context_test.py\n+++ b/tests/code_context_test.py\n@@ -215,3 +215,2 @@ async def test_max_auto_tokens(mocker, temp_testbed, mock_session_context):\n     code_context.include(\"file_1.py\")\n-    code_context.use_llm = False\n     mock_session_context.config.auto_context_tokens = 8000\n@@ -269,3 +268,2 @@ async def test_get_code_message_ignore(mocker, temp_testbed, mock_session_contex\n     )\n-    code_context.use_llm = False\n     code_message = await code_context.get_code_message(0, prompt=\"prompt\")\n",
    "test_command": "",
    "version": "0.1.0"
}