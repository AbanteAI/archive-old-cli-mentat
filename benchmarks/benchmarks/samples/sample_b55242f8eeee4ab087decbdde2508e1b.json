{
    "title": "Make LLM Filter response json, remove tries",
    "description": "",
    "id": "b55242f8eeee4ab087decbdde2508e1b",
    "parent_id": "",
    "repo": "https://github.com/AbanteAI/mentat",
    "merge_base": "9498bdd",
    "diff_merge_base": "diff --git a/mentat/code_context.py b/mentat/code_context.py\nindex ec4d207..362682f 100644\n--- a/mentat/code_context.py\n+++ b/mentat/code_context.py\n@@ -106,4 +106,2 @@ class CodeContext:\n \n-    use_llm: bool = False\n-\n     async def get_code_message(\n@@ -164,3 +162,2 @@ class CodeContext:\n                 auto_tokens,\n-                self.use_llm,\n                 prompt,\ndiff --git a/mentat/config.py b/mentat/config.py\nindex 8d52d78..4f42393 100644\n--- a/mentat/config.py\n+++ b/mentat/config.py\n@@ -118,2 +118,17 @@ class Config:\n     )\n+    llm_feature_filter: int = attr.field(  # pyright: ignore\n+        default=0,\n+        metadata={\n+            \"description\": (\n+                \"Send this many tokens of auto-context-selected code files to an LLM\"\n+                \" along with the user_prompt to post-select only files which are\"\n+                \" relevant to the task. Post-files will then be sent to the LLM again\"\n+                \" to respond to the user's prompt.\"\n+            ),\n+            \"abbreviation\": \"l\",\n+            \"const\": 5000,\n+        },\n+        converter=int,\n+        validator=validators.ge(0),  # pyright: ignore\n+    )\n \ndiff --git a/mentat/feature_filters/default_filter.py b/mentat/feature_filters/default_filter.py\nindex e2d4b74..be8f6ab 100644\n--- a/mentat/feature_filters/default_filter.py\n+++ b/mentat/feature_filters/default_filter.py\n@@ -15,3 +15,2 @@ class DefaultFilter(FeatureFilter):\n         max_tokens: int,\n-        use_llm: bool = False,\n         user_prompt: Optional[str] = None,\n@@ -21,3 +20,2 @@ class DefaultFilter(FeatureFilter):\n         self.max_tokens = max_tokens\n-        self.use_llm = use_llm\n         self.user_prompt = user_prompt or \"\"\n@@ -29,8 +27,9 @@ class DefaultFilter(FeatureFilter):\n \n+        use_llm = ctx.config.llm_feature_filter > 0\n         if ctx.config.auto_context_tokens > 0 and self.user_prompt != \"\":\n             features = await EmbeddingSimilarityFilter(\n-                self.user_prompt, (0.5 if self.use_llm else 1) * self.loading_multiplier\n+                self.user_prompt, (0.5 if use_llm else 1) * self.loading_multiplier\n             ).filter(features)\n \n-        if self.use_llm:\n+        if use_llm:\n             try:\ndiff --git a/mentat/feature_filters/llm_feature_filter.py b/mentat/feature_filters/llm_feature_filter.py\nindex 3a80c76..530a067 100644\n--- a/mentat/feature_filters/llm_feature_filter.py\n+++ b/mentat/feature_filters/llm_feature_filter.py\n@@ -54,2 +54,3 @@ class LLMFeatureFilter(FeatureFilter):\n             )\n+        context_size = min(context_size, config.llm_feature_filter)\n         system_prompt = read_prompt(self.feature_selection_prompt_path)\ndiff --git a/tests/code_context_test.py b/tests/code_context_test.py\nindex 41877d6..63808e1 100644\n--- a/tests/code_context_test.py\n+++ b/tests/code_context_test.py\n@@ -215,3 +215,2 @@ async def test_max_auto_tokens(mocker, temp_testbed, mock_session_context):\n     code_context.include(\"file_1.py\")\n-    code_context.use_llm = False\n     mock_session_context.config.auto_context_tokens = 8000\n@@ -269,3 +268,2 @@ async def test_get_code_message_ignore(mocker, temp_testbed, mock_session_contex\n     )\n-    code_context.use_llm = False\n     code_message = await code_context.get_code_message(0, prompt=\"prompt\")\ndiff --git a/tests/feature_filters/llm_feature_filter_test.py b/tests/feature_filters/llm_feature_filter_test.py\nindex db65019..f77f59d 100644\n--- a/tests/feature_filters/llm_feature_filter_test.py\n+++ b/tests/feature_filters/llm_feature_filter_test.py\n@@ -7,3 +7,5 @@ from mentat.feature_filters.llm_feature_filter import LLMFeatureFilter\n @pytest.mark.asyncio\n-async def test_llm_feature_filter(mocker, temp_testbed, mock_call_llm_api):\n+async def test_llm_feature_filter(\n+    mocker, temp_testbed, mock_call_llm_api, mock_session_context\n+):\n     all_features = [\n@@ -18,2 +20,3 @@ async def test_llm_feature_filter(mocker, temp_testbed, mock_call_llm_api):\n     mock_call_llm_api.set_unstreamed_values('[\"multifile_calculator/operations.py\"]')\n+    mock_session_context.config.llm_feature_filter = 2000\n \n",
    "diff_active": "",
    "message_history": [],
    "message_prompt": "In my LLMFeatureFilter, when I call the llm api, include a response_format type=\"json_object\". Then, get rid of the n_tries loop/comments.",
    "message_edit": "I will update the `LLMFeatureFilter` class in the `mentat/feature_filters/llm_feature_filter.py` file to include a `response_format` with `type=\"json_object\"` when calling the LLM API. I will also remove the `n_tries` loop and associated comments.\n\nSteps:\n1. Update the `call_llm_api` method call to include `response_format=ResponseFormat(type=\"json_object\")`.\n2. Remove the `n_tries` loop and associated comments.",
    "context": [
        "mentat/feature_filters/llm_feature_filter.py",
        "mentat/llm_api_handler.py"
    ],
    "diff_edit": "diff --git a/mentat/feature_filters/llm_feature_filter.py b/mentat/feature_filters/llm_feature_filter.py\nindex 530a067..af93177 100644\n--- a/mentat/feature_filters/llm_feature_filter.py\n+++ b/mentat/feature_filters/llm_feature_filter.py\n@@ -10,2 +10,3 @@ from openai.types.chat import (\n )\n+from openai.types.chat.completion_create_params import ResponseFormat\n \n@@ -103,28 +104,24 @@ class LLMFeatureFilter(FeatureFilter):\n         selected_refs = list[Path]()\n-        n_tries = 3\n-        # TODO: When we switch to JSON format and don't have to try multiple times,\n-        # use cost_tracker.display_last_api_call to show cost after loading bar disappears\n-        for i in range(n_tries):\n-            start_time = default_timer()\n-            llm_response = await llm_api_handler.call_llm_api(\n-                messages, model, stream=False\n-            )\n-            message = (llm_response.choices[0].message.content) or \"\"\n+        start_time = default_timer()\n+        llm_response = await llm_api_handler.call_llm_api(\n+            messages,\n+            model,\n+            stream=False,\n+            response_format=ResponseFormat(type=\"json_object\"),\n+        )\n+        message = (llm_response.choices[0].message.content) or \"\"\n \n-            tokens = prompt_tokens(messages, model)\n-            response_tokens = count_tokens(message, model, full_message=True)\n-            cost_tracker.log_api_call_stats(\n-                tokens,\n-                response_tokens,\n-                model,\n-                default_timer() - start_time,\n-            )\n-            try:\n-                response = json.loads(message)  # type: ignore\n-                selected_refs = [Path(r) for r in response]\n-                break\n-            except json.JSONDecodeError:\n-                # TODO: Update Loader\n-                if i == n_tries - 1:\n-                    raise ModelError(f\"The response is not valid json: {message}\")\n+        tokens = prompt_tokens(messages, model)\n+        response_tokens = count_tokens(message, model, full_message=True)\n+        cost_tracker.log_api_call_stats(\n+            tokens,\n+            response_tokens,\n+            model,\n+            default_timer() - start_time,\n+        )\n+        try:\n+            response = json.loads(message)  # type: ignore\n+            selected_refs = [Path(r) for r in response]\n+        except json.JSONDecodeError:\n+            raise ModelError(f\"The response is not valid json: {message}\")\n         if self.loading_multiplier:\n",
    "test_command": "",
    "version": "0.1.0"
}