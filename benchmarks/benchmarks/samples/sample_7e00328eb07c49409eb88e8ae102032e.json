{
    "title": "Import config and use config fixture in test",
    "description": "",
    "id": "7e00328eb07c49409eb88e8ae102032e",
    "parent_id": "",
    "repo": "https://github.com/AbanteAI/mentat",
    "merge_base": "9498bdde0dcdeafcaaac6ab148bdb599f3d7548c",
    "diff_merge_base": "",
    "diff_active": "diff --git a/mentat/code_context.py b/mentat/code_context.py\nindex ec4d207..362682f 100644\n--- a/mentat/code_context.py\n+++ b/mentat/code_context.py\n@@ -106,4 +106,2 @@ class CodeContext:\n \n-    use_llm: bool = False\n-\n     async def get_code_message(\n@@ -164,3 +162,2 @@ class CodeContext:\n                 auto_tokens,\n-                self.use_llm,\n                 prompt,\ndiff --git a/mentat/config.py b/mentat/config.py\nindex 8d52d78..4f42393 100644\n--- a/mentat/config.py\n+++ b/mentat/config.py\n@@ -118,2 +118,17 @@ class Config:\n     )\n+    llm_feature_filter: int = attr.field(  # pyright: ignore\n+        default=0,\n+        metadata={\n+            \"description\": (\n+                \"Send this many tokens of auto-context-selected code files to an LLM\"\n+                \" along with the user_prompt to post-select only files which are\"\n+                \" relevant to the task. Post-files will then be sent to the LLM again\"\n+                \" to respond to the user's prompt.\"\n+            ),\n+            \"abbreviation\": \"l\",\n+            \"const\": 5000,\n+        },\n+        converter=int,\n+        validator=validators.ge(0),  # pyright: ignore\n+    )\n \ndiff --git a/mentat/feature_filters/default_filter.py b/mentat/feature_filters/default_filter.py\nindex e2d4b74..be8f6ab 100644\n--- a/mentat/feature_filters/default_filter.py\n+++ b/mentat/feature_filters/default_filter.py\n@@ -15,3 +15,2 @@ class DefaultFilter(FeatureFilter):\n         max_tokens: int,\n-        use_llm: bool = False,\n         user_prompt: Optional[str] = None,\n@@ -21,3 +20,2 @@ class DefaultFilter(FeatureFilter):\n         self.max_tokens = max_tokens\n-        self.use_llm = use_llm\n         self.user_prompt = user_prompt or \"\"\n@@ -29,8 +27,9 @@ class DefaultFilter(FeatureFilter):\n \n+        use_llm = ctx.config.llm_feature_filter > 0\n         if ctx.config.auto_context_tokens > 0 and self.user_prompt != \"\":\n             features = await EmbeddingSimilarityFilter(\n-                self.user_prompt, (0.5 if self.use_llm else 1) * self.loading_multiplier\n+                self.user_prompt, (0.5 if use_llm else 1) * self.loading_multiplier\n             ).filter(features)\n \n-        if self.use_llm:\n+        if use_llm:\n             try:\ndiff --git a/mentat/feature_filters/llm_feature_filter.py b/mentat/feature_filters/llm_feature_filter.py\nindex 3a80c76..530a067 100644\n--- a/mentat/feature_filters/llm_feature_filter.py\n+++ b/mentat/feature_filters/llm_feature_filter.py\n@@ -54,2 +54,3 @@ class LLMFeatureFilter(FeatureFilter):\n             )\n+        context_size = min(context_size, config.llm_feature_filter)\n         system_prompt = read_prompt(self.feature_selection_prompt_path)\ndiff --git a/tests/code_context_test.py b/tests/code_context_test.py\nindex 41877d6..63808e1 100644\n--- a/tests/code_context_test.py\n+++ b/tests/code_context_test.py\n@@ -215,3 +215,2 @@ async def test_max_auto_tokens(mocker, temp_testbed, mock_session_context):\n     code_context.include(\"file_1.py\")\n-    code_context.use_llm = False\n     mock_session_context.config.auto_context_tokens = 8000\n@@ -269,3 +268,2 @@ async def test_get_code_message_ignore(mocker, temp_testbed, mock_session_contex\n     )\n-    code_context.use_llm = False\n     code_message = await code_context.get_code_message(0, prompt=\"prompt\")\n",
    "message_history": [],
    "message_prompt": "Update my llm feature filter test now that it uses a config variable. Import mock_session_context and set config.llm_feature_filter to 2000 before you setup the filter.",
    "message_edit": "I will update the `test_llm_feature_filter` test case in `tests/feature_filters/llm_feature_filter_test.py` to use the `mock_session_context` fixture and set `config.llm_feature_filter` to 2000 before setting up the `LLMFeatureFilter`.",
    "context": [
        "mentat/config.py",
        "tests/feature_filters/llm_feature_filter_test.py",
        "mentat/code_context.py",
        "mentat/feature_filters/default_filter.py",
        "mentat/feature_filters/llm_feature_filter.py",
        "tests/code_context_test.py",
        "tests/conftest.py"
    ],
    "diff_edit": "diff --git a/tests/feature_filters/llm_feature_filter_test.py b/tests/feature_filters/llm_feature_filter_test.py\nindex db65019..f77f59d 100644\n--- a/tests/feature_filters/llm_feature_filter_test.py\n+++ b/tests/feature_filters/llm_feature_filter_test.py\n@@ -7,3 +7,5 @@ from mentat.feature_filters.llm_feature_filter import LLMFeatureFilter\n @pytest.mark.asyncio\n-async def test_llm_feature_filter(mocker, temp_testbed, mock_call_llm_api):\n+async def test_llm_feature_filter(\n+    mocker, temp_testbed, mock_call_llm_api, mock_session_context\n+):\n     all_features = [\n@@ -18,2 +20,3 @@ async def test_llm_feature_filter(mocker, temp_testbed, mock_call_llm_api):\n     mock_call_llm_api.set_unstreamed_values('[\"multifile_calculator/operations.py\"]')\n+    mock_session_context.config.llm_feature_filter = 2000\n \n",
    "test_command": "",
    "version": "0.1.0"
}
